{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4da947",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We're going to do a visual comparison of the ProtHash and ESMC embeddings as a sanity check. We're expecting that, if ProtHash successfully learned from its ESMC teacher, the two embeddings will be nearly identical in terms of their 2D plots. Now it's not exactly comparing apples to apples when you have to take two high-dimensional embeddings of likely different dimensions and reduce them both down to only two dimensions - but this is just a sanity check.\n",
    "\n",
    "Let's kick this party off by defining some configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb3f0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available as cuda_is_available\n",
    "from torch.backends.mps import is_available as mps_is_available\n",
    "\n",
    "min_sequence_length=1\n",
    "max_sequence_length=2048\n",
    "num_samples=1000\n",
    "batch_size=32\n",
    "\n",
    "teacher_model_name=\"esmc_300m\"\n",
    "\n",
    "checkpoint_path=\"checkpoints/checkpoint.pt\"\n",
    "\n",
    "device=\"cuda\" if cuda_is_available() else \"mps\" if mps_is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d7588",
   "metadata": {},
   "source": [
    "Then, we'll load the ESM protein sequence tokenizer and the SwissProt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161ab08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.tokenization import EsmSequenceTokenizer\n",
    "\n",
    "from data import SwissProt\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "tokenizer = EsmSequenceTokenizer()\n",
    "\n",
    "dataset = SwissProt(\n",
    "    tokenizer=tokenizer,\n",
    "    min_sequence_length=min_sequence_length,\n",
    "    max_sequence_length=max_sequence_length,\n",
    ")\n",
    "\n",
    "dataset = Subset(dataset, range(num_samples))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, collate_fn=dataset.dataset.collate_pad_right\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f23b7",
   "metadata": {},
   "source": [
    "Next we'll load the teacher model, ESMC, from its pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d3050e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f915c9a1b9e487a9079de9792624c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "\n",
    "teacher = ESMC.from_pretrained(teacher_model_name)\n",
    "\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "print(\"Teacher model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed99f5",
   "metadata": {},
   "source": [
    "Now you've made it this far it's time for some fun. Let's go down and dirty and load one of our ProtHash model checkpoints into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57ffc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ProtHash:\n\tUnexpected key(s) in state_dict: \"head.linear.weight\", \"head.linear.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m checkpoint = torch.load(checkpoint_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      7\u001b[39m student = ProtHash(**checkpoint[\u001b[33m\"\u001b[39m\u001b[33mmodel_args\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mstudent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m student.remove_adapter_head()\n\u001b[32m     13\u001b[39m student = student.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/ProtHash/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ProtHash:\n\tUnexpected key(s) in state_dict: \"head.linear.weight\", \"head.linear.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from src.prothash.model import ProtHash\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "student = ProtHash(**checkpoint[\"model_args\"])\n",
    "\n",
    "\n",
    "\n",
    "student.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "student.remove_adapter_head()\n",
    "\n",
    "student = student.to(device)\n",
    "\n",
    "student.eval()\n",
    "\n",
    "print(\"Model checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ea80e",
   "metadata": {},
   "source": [
    "You've made it this far there's no turning back. It's literally life or death from here on out. Next we'll be embedding a subset of the SwissProt dataset with both models. I'll know if you turned back from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_embeddings = []\n",
    "teacher_embeddings = []\n",
    "\n",
    "for x in dataloader:\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_teacher = teacher.forward(x)\n",
    "        y_teacher = out_teacher.hidden_states[-1][:, 0, :]\n",
    "\n",
    "    y_student = student.forward(x)\n",
    "\n",
    "    student_embeddings.append(y_student.cpu())\n",
    "    teacher_embeddings.append(y_teacher.cpu())\n",
    "\n",
    "assert len(student_embeddings) == len(teacher_embeddings)\n",
    "\n",
    "student_embeddings = torch.cat(student_embeddings, dim=0)\n",
    "teacher_embeddings = torch.cat(teacher_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623cf1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=32)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "s = student_embeddings.numpy()\n",
    "t = teacher_embeddings.numpy()\n",
    "\n",
    "s = pca.fit_transform(s)\n",
    "t = pca.fit_transform(t)\n",
    "\n",
    "s = tsne.fit_transform(s)\n",
    "t = tsne.fit_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].scatter(s[:, 0], s[:, 1], s=5, alpha=0.7)\n",
    "axes[0].set_title('Student embeddings (2D)')\n",
    "axes[1].scatter(t[:, 0], t[:, 1], s=5, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Teacher embeddings (2D)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('dim 1')\n",
    "    ax.set_ylabel('dim 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
