{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4da947",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We're going to do a visual comparison of the ProtHash and ESMC embeddings as a sanity check. We're expecting that, if ProtHash successfully learned from its ESMC teacher, the two embeddings will be nearly identical in terms of their 2D plots. Now it's not exactly comparing apples to apples when you have to take two high-dimensional embeddings of likely different dimensions and reduce them both down to only two dimensions - but this is just a sanity check.\n",
    "\n",
    "Let's kick this party off by defining some configuration variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f0812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda import is_available as cuda_is_available\n",
    "from torch.backends.mps import is_available as mps_is_available\n",
    "\n",
    "min_sequence_length=1\n",
    "max_sequence_length=2048\n",
    "num_samples=1000\n",
    "batch_size=32\n",
    "\n",
    "teacher_model_name=\"esmc_300m\"\n",
    "\n",
    "checkpoint_path=\"checkpoints/checkpoint.pt\"\n",
    "\n",
    "device=\"cuda\" if cuda_is_available() else \"mps\" if mps_is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d7588",
   "metadata": {},
   "source": [
    "Then, we'll load the ESM protein sequence tokenizer and the SwissProt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.tokenization import EsmSequenceTokenizer\n",
    "\n",
    "from data import SwissProt\n",
    "\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "tokenizer = EsmSequenceTokenizer()\n",
    "\n",
    "dataset = SwissProt(\n",
    "    tokenizer=tokenizer,\n",
    "    min_sequence_length=min_sequence_length,\n",
    "    max_sequence_length=max_sequence_length,\n",
    ")\n",
    "\n",
    "dataset = Subset(dataset, range(num_samples))\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=False, collate_fn=dataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5f23b7",
   "metadata": {},
   "source": [
    "Next we'll load the teacher model, ESMC, from its pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3050e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from esm.models.esmc import ESMC\n",
    "\n",
    "teacher = ESMC.from_pretrained(teacher_model_name)\n",
    "\n",
    "teacher = teacher.to(device)\n",
    "\n",
    "teacher.eval()\n",
    "\n",
    "print(\"Teacher model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ed99f5",
   "metadata": {},
   "source": [
    "Now you've made it this far it's time for some fun. Let's go down and dirty and load one of our ProtHash model checkpoints into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc57ffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.prothash.model import ProtHash\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n",
    "\n",
    "student = ProtHash(**checkpoint[\"model_args\"])\n",
    "\n",
    "student.remove_adapter_head()\n",
    "\n",
    "student.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "student = student.to(device)\n",
    "\n",
    "student.eval()\n",
    "\n",
    "print(\"Model checkpoint loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3ea80e",
   "metadata": {},
   "source": [
    "You've made it this far there's no turning back. It's literally life or death from here on out. Next we'll be embedding a subset of the SwissProt dataset with both models. I'll know if you turned back from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_embeddings = []\n",
    "teacher_embeddings = []\n",
    "\n",
    "for x in dataloader:\n",
    "    x = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_teacher = teacher.forward(x)\n",
    "        y_teacher = out_teacher.hidden_states[-1][:, 0, :]\n",
    "\n",
    "    y_student = student.forward(x)\n",
    "\n",
    "    student_embeddings.append(y_student.cpu())\n",
    "    teacher_embeddings.append(y_teacher.cpu())\n",
    "\n",
    "assert len(student_embeddings) == len(teacher_embeddings)\n",
    "\n",
    "student_embeddings = torch.cat(student_embeddings, dim=0)\n",
    "teacher_embeddings = torch.cat(teacher_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623cf1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e9868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pca = PCA(n_components=32)\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "\n",
    "student_embeddings = pca.fit_transform(student_embeddings)\n",
    "teacher_embeddings = pca.fit_transform(teacher_embeddings)\n",
    "\n",
    "student_embeddings = tsne.fit_transform(student_embeddings)\n",
    "teacher_embeddings = tsne.fit_transform(teacher_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b9b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "s = student_embeddings.numpy()\n",
    "t = teacher_embeddings.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].scatter(s[:, 0], s[:, 1], s=5, alpha=0.7)\n",
    "axes[0].set_title('Student embeddings (2D)')\n",
    "axes[1].scatter(t[:, 0], t[:, 1], s=5, alpha=0.7, color='orange')\n",
    "axes[1].set_title('Teacher embeddings (2D)')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('dim 1')\n",
    "    ax.set_ylabel('dim 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
